{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End to end fine tuning Notebook\n",
    "\n",
    "1. data preparation\n",
    "2. tokenization\n",
    "3. fine tuning with QLora\n",
    "3. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages installation and key obj instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install huggingface_hub --upgrade --quiet\n",
    "!pip install \"transformers==4.30.2\" \"datasets[s3]==2.13.0\" sagemaker --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are going to use Sagemaker in a local environment. You need access to an IAM Role with the required permissions for Sagemaker. You can find [here](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) more about it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#required to work in local_mode on your notebook instance for development/debugging purpose\n",
    "#!pip install 'sagemaker[local]' --upgrade --quiet\n",
    "#!pip install docker-compose --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::327216439222:role/Sagemaker\n",
      "sagemaker bucket: sagemaker-us-east-1-327216439222\n",
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "import os\n",
    "\n",
    "#uncomment to run in local mode\n",
    "#from sagemaker import LocalSession\n",
    "#sess = LocalSession()\n",
    "#the below help setting up the container's root on the EBS volume of your instance.\n",
    "#sess.config = {'local' : {'local_code' : True, 'container_root' : '/home/ec2-user/SageMaker/'}}\n",
    "#if you're running local mode and run into out of space issues, consider running docker_scripts/prepare-docker.sh to set the docker root under /home/ec2-user/SageMaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "region = sess.boto_region_name\n",
    "\n",
    "#replace the below by a specific bucket if you need\n",
    "sagemaker_session_bucket = sess.default_bucket()\n",
    "role = sagemaker.get_execution_role()\n",
    "s3_client = boto3.client(\"s3\")\n",
    "s3_prefix = \"model-fine-tuning\"\n",
    "\n",
    "#local notebook path\n",
    "notebook_home = \"/home/ec2-user/SageMaker/\"\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {region}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model selection\n",
    "\n",
    "Choose the model you want to fine tune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id = \"tiiuae/falcon-7b\"\n",
    "#model_id = \"tiiuae/falcon-7b-instruct\"\n",
    "model_name = model_id.split(\"/\")[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structure of our BBC Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're using BBC articles for our fine tuning contained in the local zip file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "#name fo the zip file that we'll use\n",
    "data_zip = \"BBC_news_summary.zip\"\n",
    "\n",
    "base_dir = os.path.join(os.getcwd())\n",
    "\n",
    "path_to_file = os.path.join(os.getcwd(), \"data\", data_zip)\n",
    "\n",
    "#unziping file\n",
    "with zipfile.ZipFile(os.path.join(base_dir, \"data\", data_zip), 'r') as zip_ref:\n",
    "    zip_ref.extractall(os.path.join(notebook_home, \"data\"))\n",
    "\n",
    "#Folders that we'll iterate through after unzipping.\n",
    "articles_folder = \"News Articles\"\n",
    "summaries_folder = \"Summaries\"\n",
    "sub_folders = [\"business\", \"entertainment\", \"politics\", \"sport\", \"tech\"]\n",
    "\n",
    "articles_folders = f\"{notebook_home}/data/BBC_news_summary/\" + articles_folder\n",
    "summaries_folder = f\"{notebook_home}/data/BBC_news_summary/\" + summaries_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform folder base data into jsonlines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See below the format that we want:\n",
    "\n",
    "{\n",
    "\n",
    "  \"id\": \"13818513\",\n",
    "  \n",
    "  \"summary\": \"Amanda baked cookies and will bring Jerry some tomorrow.\",\n",
    "  \n",
    "  \"content\": \"Amanda: I baked cookies. Do you want some?\\r\\nJerry: Sure!\\r\\nAmanda: I'll bring you tomorrow :-)\"\n",
    "  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping:sport_199 due to UnicodeDecodeError\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(os.path.join(notebook_home, \"data\", \"data_jsonlines.jsonl\"), 'w') as outfile:\n",
    "    for folder in os.scandir(path = articles_folders):\n",
    "        for filename in os.scandir(path = articles_folders + \"/\" + str(folder.name)):\n",
    "            if filename.is_file():\n",
    "                try:\n",
    "                    #create article id of the form folder_001\n",
    "                    id_article = str(folder.name) + \"_\" + str(filename.name).split(\".\")[0]\n",
    "\n",
    "                    #get article content\n",
    "                    content = \"\"\n",
    "                    with open(filename, 'rb') as file:\n",
    "                        content = file.read()\n",
    "                    #get article summary\n",
    "                    summary = \"\"\n",
    "                    equivalent_summary_file = summaries_folder + \"/\" + str(folder.name) + \"/\" + str(filename.name)\n",
    "                    with open(equivalent_summary_file, 'rb') as file:\n",
    "                        summary = file.read()\n",
    "\n",
    "                    #create json object\n",
    "                    data = {}\n",
    "                    data['id'] = id_article\n",
    "                    data['content'] = content.decode(\"utf-8\")\n",
    "                    data['summary'] = summary.decode(\"utf-8\")\n",
    "   \n",
    "                    json.dump(data, outfile)\n",
    "                    outfile.write('\\n')\n",
    "\n",
    "                except UnicodeDecodeError:\n",
    "                    print(f\"skipping:{id_article} due to UnicodeDecodeError\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split test train dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we're splitting the jsonl file into train and test before further processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "with open(os.path.join(notebook_home, \"data\", \"data_jsonlines.jsonl\")) as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "train, test = train_test_split(lines, test_size=0.2)\n",
    "\n",
    "with open(os.path.join(notebook_home, \"data\", \"data_jsonlines_train.jsonl\"), 'w') as outfile:\n",
    "    for t in train:\n",
    "        #no need to add an escape character as there is already one.\n",
    "        outfile.write(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_data = []\n",
    "for t in test:\n",
    "    t = json.loads(t)\n",
    "    #payload = {\"inputs\": t[\"content\"], \"parameters\":{ \"do_sample\": True, \"top_p\": 0.9, \"temperature\": 0.3, \"max_new_tokens\": 1024}}\n",
    "    payload = {\"inputs\": f'{t[\"content\"]}'}\n",
    "    test_data.append(payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(notebook_home, \"data\", \"data_jsonlines_test.jsonl\"), 'w') as outfile:\n",
    "    for t in test_data:\n",
    "        json.dump(t, outfile)\n",
    "        outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload the test jsonline file for later test/validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-1-327216439222/model-fine-tuning/test/data_jsonlines_test.jsonl'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3_client.upload_file(os.path.join(notebook_home, \"data\", \"data_jsonlines_test.jsonl\"), sagemaker_session_bucket, os.path.join(s3_prefix, \"test\", \"data_jsonlines_test.jsonl\"))\n",
    "test_input_path = os.path.join(\"s3://\", sagemaker_session_bucket, s3_prefix, \"test\", \"data_jsonlines_test.jsonl\")\n",
    "test_input_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load into HF dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to /home/ec2-user/.cache/huggingface/datasets/json/default-bfa0b3dcf2f75f20/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4677fce1be1346af92fa080681f7a042",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab297bc5907d483f8c9f96b25f71f067",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /home/ec2-user/.cache/huggingface/datasets/json/default-bfa0b3dcf2f75f20/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, load_from_disk\n",
    "\n",
    "#we load the data into a dataset object\n",
    "dataset = load_dataset('json', data_files=os.path.join(notebook_home, \"data\", \"data_jsonlines_train.jsonl\"), split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'content', 'summary'],\n",
       "    num_rows: 1779\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the template to the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation for Domain adaptation\n",
    "\n",
    "we prepare the data for Domain adaptation. in that scenario we're merging content and summary into one \"text\" feature as we only care about the \"language modelling value\" of that training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e98a397af50f4ce9853bee3d6125bf5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/220 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba9050774a1d4a389a99fbd306b21d66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.73M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7436bf39aa6642bc8b8bd079f3f312b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/281 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load tokenizer of falcon\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.model_max_length = 2048 #you might want to reduce that depending on GPU memory available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from random import randint\n",
    "from itertools import chain\n",
    "from functools import partial\n",
    "\n",
    "# custom instruct prompt start\n",
    "prompt_template_domain = f\"Provide a summary for the following article:\\n{{content}}\\n---\\nSummary:\\n{{summary}}{{eos_token}}\"\n",
    "\n",
    "# template dataset to add prompt to each sample\n",
    "def template_dataset_domain_tuning(sample):\n",
    "    sample[\"text\"] = prompt_template_domain.format(content=sample[\"content\"],\n",
    "                                            summary=sample[\"summary\"],\n",
    "                                            eos_token=tokenizer.eos_token)\n",
    "    return sample\n",
    "\n",
    "# apply prompt template per sample\n",
    "\n",
    "dataset_domain = dataset.map(template_dataset_domain_tuning, remove_columns=list(dataset.features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 1779\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Provide a summary for the following article:\n",
      "Tsunami slows Sri Lanka's growth\n",
      "\n",
      "Sri Lanka's president has launched a reconstruction drive worth $3.5bn (£1.8bn) by appealing for peace and national unity.\n",
      "\n",
      "President Kumaratunga said it was now important to find a peaceful solution to years of internal conflict. Meanwhile, the International Monetary Fund (IMF) said damage from the tsunami would cut one percentage point from Sri Lanka's economic growth this year. It estimated the wave left physical damage equal to 6.5% of the economy.\n",
      "\n",
      "Separately, the International Labour Organisation (ILO) said that at least one million people have lost their livelihoods in Sri Lanka and Indonesia alone. It called for action to create jobs. President Kumaratunga attended a ceremony in the southern town of Hambantota. She was joined by government and opposition politicians, together with Buddhist, Hindu, Muslim and Christian clergy.\n",
      "\n",
      "Prime Minister Mahinda Rajapakse laid the foundation stone on a new housing project intended to provide 6,000 homes for survivors of the tsunami. Mrs Kumaratunga called for the tragedy to be \"the start of a new beginning to rebuild our nation\". \"We are a country blessed with so many natural resources and we have not made use of them fully. Instead we have been squabbling, fighting,\" she added. Norway's peace negotiator Erik Solheim is due to arrive on Wednesday to try to revive peace talks in the decades-long conflict between government forces and the Tamil Tigers, who want a separate state in the north east of the country. Reconstruction efforts in eastern Sri Lanka have been hampered by tensions between the two sides.\n",
      "\n",
      "The IMF said that the Sri Lankan authorities' initial estimates have put the physical damage at $1.3 to $1.5bn, but added that the implications for the economy were much wider than this. \"The broader macroeconomic impact will clearly be substantial but the details are difficult to assess at this early stage,\" the IMF said. Growth, inflation, the balance of payments and foreign exchange reserves are all expected to show the effects of lost businesses and reconstruction costs. \"The fishing industry has been devastated, agricultural production may be affected and tourism will suffer, especially in the short term,\" the report said. The ILO estimated that 400,000 Sri Lankans have lost their jobs, mostly in these three industries. Earnings from tourism this year are expected to be 15% lower than last year. Economic growth this year is expected to be 4%, which is about 1% less than previously forecast. Inflation could climb to 14% compared to a previous estimate of 12%. Although major exports have not suffered, the IMF expects the reconstruction effort will require higher imports which could damage the balance of payments. Foreign exchange reserves may become strained as \"Sri Lanka will be hard pressed to keep international reserves at the pre-tsunami level\" which totalled more than two months worth of imports. Last week, the IMF approved Sri Lanka's request for a freeze on loan repayments.\n",
      "\n",
      "---\n",
      "Summary:\n",
      "Meanwhile, the International Monetary Fund (IMF) said damage from the tsunami would cut one percentage point from Sri Lanka's economic growth this year.The IMF said that the Sri Lankan authorities' initial estimates have put the physical damage at $1.3 to $1.5bn, but added that the implications for the economy were much wider than this.President Kumaratunga said it was now important to find a peaceful solution to years of internal conflict.Sri Lanka's president has launched a reconstruction drive worth $3.5bn (£1.8bn) by appealing for peace and national unity.Separately, the International Labour Organisation (ILO) said that at least one million people have lost their livelihoods in Sri Lanka and Indonesia alone.The ILO estimated that 400,000 Sri Lankans have lost their jobs, mostly in these three industries.Reconstruction efforts in eastern Sri Lanka have been hampered by tensions between the two sides.Last week, the IMF approved Sri Lanka's request for a freeze on loan repayments.Growth, inflation, the balance of payments and foreign exchange reserves are all expected to show the effects of lost businesses and reconstruction costs.Earnings from tourism this year are expected to be 15% lower than last year.<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "#printing an example for domain fine tuning data\n",
    "print(dataset_domain[randint(0, len(dataset))][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer and\" chunking\"\n",
    "\n",
    "we retrieve the tokenizer for our specific model using the very convenient HF from_pretrained() API.\n",
    "\n",
    "we then concatenate the text in chunk of a certain size. we don't care about chunks containing 2 parts of the same article, we only care about not losing information and providing training data of the same format for the model to be trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# empty list to save remainder from batches to use in next batch\n",
    "remainder = {\"input_ids\": [], \"token_type_ids\": [], \"attention_mask\": []}\n",
    "\n",
    "def chunk(sample, chunk_length=2048):\n",
    "    # define global remainder variable to save remainder from batches to use in next batch\n",
    "    global remainder\n",
    "    # Concatenate all texts and add remainder from previous batch\n",
    "    concatenated_examples = {k: list(chain(*sample[k])) for k in sample.keys()}\n",
    "    #print(concatenated_examples.keys())\n",
    "    concatenated_examples = {k: remainder[k] + concatenated_examples[k] for k in concatenated_examples.keys()}\n",
    "    # get total number of tokens for batch\n",
    "    batch_total_length = len(concatenated_examples[list(sample.keys())[0]])\n",
    "\n",
    "    # get max number of chunks for batch\n",
    "    if batch_total_length >= chunk_length:\n",
    "        batch_chunk_length = (batch_total_length // chunk_length) * chunk_length\n",
    "\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + chunk_length] for i in range(0, batch_chunk_length, chunk_length)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    # add remainder to global variable for next batch\n",
    "    remainder = {k: concatenated_examples[k][batch_chunk_length:] for k in concatenated_examples.keys()}\n",
    "    # prepare labels\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "# tokenize and chunk dataset\n",
    "def tokenize_chunk(dataset, tokenizer):\n",
    "    lm_dataset = dataset.map(\n",
    "        lambda sample: tokenizer(sample[\"text\"]), batched=True, remove_columns=list(dataset.features)\n",
    "    ).map(\n",
    "        partial(chunk, chunk_length=2048),\n",
    "        batched=True,\n",
    "    )\n",
    "    return lm_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2555 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples: 640\n"
     ]
    }
   ],
   "source": [
    "# tokenize and chunk dataset for Instruction dataset\n",
    "lm_dataset_domain = tokenize_chunk(dataset_domain, tokenizer)\n",
    "\n",
    "# Print total number of samples\n",
    "print(f\"Total number of samples: {len(lm_dataset_domain)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenizer transformed our \"text\" feature into a tokenized version compatible with our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 640\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_dataset_domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[47945, 241, 11055, 312, 248, 1863, 2507, 37, 193, 6273, 424, 14522, 648, 204, 18, 55298, 5676, 18, 193, 193, 52096, 5485, 2821, 4562, 271, 980, 4881, 3824, 11701, 312, 10629, 272, 248, 2574, 6675, 23, 1815, 10374, 47958, 25, 193, 193, 44, 6414, 312, 248, 13730, 8305, 204, 6186, 16, 275, 648, 24, 3690, 94, 1112, 506, 1304, 5676, 272, 241, 2679, 6675, 4354, 335, 645, 204, 1392, 16, 275, 204, 1121, 271, 204, 1463, 649, 39763, 25, 35051, 9744, 17808, 393, 678, 1342, 746, 565, 241, 204, 13, 10129, 54763, 10137, 13, 398, 11272, 388, 248, 6675, 334]\n"
     ]
    }
   ],
   "source": [
    "#sample of the input_ids feature\n",
    "print(lm_dataset_domain[0]['input_ids'][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "#sample of the token_type_ids feature\n",
    "print(lm_dataset_domain[0]['token_type_ids'][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "#sample of the token_type_ids feature\n",
    "print(lm_dataset_domain[0]['attention_mask'][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[47945, 241, 11055, 312, 248, 1863, 2507, 37, 193, 6273, 424, 14522, 648, 204, 18, 55298, 5676, 18, 193, 193, 52096, 5485, 2821, 4562, 271, 980, 4881, 3824, 11701, 312, 10629, 272, 248, 2574, 6675, 23, 1815, 10374, 47958, 25, 193, 193, 44, 6414, 312, 248, 13730, 8305, 204, 6186, 16, 275, 648, 24, 3690, 94, 1112, 506, 1304, 5676, 272, 241, 2679, 6675, 4354, 335, 645, 204, 1392, 16, 275, 204, 1121, 271, 204, 1463, 649, 39763, 25, 35051, 9744, 17808, 393, 678, 1342, 746, 565, 241, 204, 13, 10129, 54763, 10137, 13, 398, 11272, 388, 248, 6675, 334]\n"
     ]
    }
   ],
   "source": [
    "#sample of the token_type_ids feature\n",
    "print(lm_dataset_domain[0]['labels'][:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you'll notice that our input_ids have the same dimensions which is the max chunk length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2048\n",
      "2048\n"
     ]
    }
   ],
   "source": [
    "print(len(lm_dataset_domain[0]['input_ids']))\n",
    "print(len(lm_dataset_domain[1]['input_ids']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation for Instruct fine tuning\n",
    "Now we take the same data but transform it differently to instruct fine tune our model for summarisation.\n",
    "Notice that this time we want each input to be truncated if above limit and not mixed across. we add padding too for data points who are under the limit to keep the length of the input constant for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load tokenizer of falcon\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.model_max_length = 2048 #you might want to reduce that depending on GPU memory available\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from random import randint\n",
    "from itertools import chain\n",
    "from functools import partial\n",
    "    \n",
    "def format_dataset_instruction_tuning(sample):\n",
    "    \n",
    "    prompt_template_instruction = f\"prompt: Provide a summary for the following text article:. Text: {{content}} completion:{{summary}}\"\n",
    "    full_prompt = prompt_template_instruction.format(content=sample[\"content\"], summary= sample[\"summary\"])\n",
    "    \n",
    "    #note that we're asking the tokenizer to do padding till max length of our chunks and also truncate if it's above the limit.\n",
    "    return tokenizer(full_prompt, padding='max_length', truncation=True, max_length=2048)\n",
    "\n",
    "def add_labels_column(sample):\n",
    "    return {'labels': sample['input_ids']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tokenize the dataset\n",
    "lm_dataset_instruction = dataset.shuffle().map(format_dataset_instruction_tuning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we're missing a labels column that is required and that is just going to be a copy of our input_ids for that scenario.\n",
    "Note as well that you can achieve similar result by adding a DataCollatorForLanguageModeling data_collator as a configuration of the Trainer object in the run_clm.py file instead of using the default one.\n",
    "\n",
    "    trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=dataset,\n",
    "            data_collator=default_data_collator,\n",
    "            #data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    "    )\n",
    "    \n",
    "More info here: https://huggingface.co/docs/transformers/main/en/main_classes/data_collator#transformers.DataCollatorForLanguageModeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lm_dataset_instruction = lm_dataset_instruction.map(add_labels_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'content', 'summary', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 1779\n",
       "})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_dataset_instruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print a row from the dataset to understand what happened and also make sure that we've got consistent chunk size across our data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': Value(dtype='string', id=None), 'content': Value(dtype='string', id=None), 'summary': Value(dtype='string', id=None), 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'token_type_ids': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None), 'labels': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None)}\n",
      "(1779, 7)\n"
     ]
    }
   ],
   "source": [
    "print(lm_dataset_instruction.features)\n",
    "print(lm_dataset_instruction.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "business_481\n",
      "Christmas sales worst since 1981\n",
      "\n",
      "UK retail sales fell in December, failing to meet expectations and making it by some counts the worst Christmas since 1981.\n",
      "\n",
      "Retail sales dropped by 1% on the month in December, after a 0.6% rise in November, the Office for National Statistics (ONS) said. The ONS revised the annual 2004 rate of growth down from the 5.9% estimated in November to 3.2%. A number of retailers have already reported poor figures for December. Clothing retailers and non-specialist stores were the worst hit with only internet retailers showing any significant growth, according to the ONS.\n",
      "\n",
      "The last time retailers endured a tougher Christmas was 23 years previously, when sales plunged 1.7%.\n",
      "\n",
      "The ONS echoed an earlier caution from Bank of England governor Mervyn King not to read too much into the poor December figures. Some analysts put a positive gloss on the figures, pointing out that the non-seasonally-adjusted figures showed a performance comparable with 2003. The November-December jump last year was roughly comparable with recent averages, although some way below the serious booms seen in the 1990s. And figures for retail volume outperformed measures of actual spending, an indication that consumers are looking for bargains, and retailers are cutting their prices.\n",
      "\n",
      "However, reports from some High Street retailers highlight the weakness of the sector. Morrisons, Woolworths, House of Fraser, Marks & Spencer and Big Food all said that the festive period was disappointing.\n",
      "\n",
      "And a British Retail Consortium survey found that Christmas 2004 was the worst for 10 years. Yet, other retailers - including HMV, Monsoon, Jessops, Body Shop and Tesco - reported that festive sales were well up on last year. Investec chief economist Philip Shaw said he did not expect the poor retail figures to have any immediate effect on interest rates. \"The retail sales figures are very weak, but as Bank of England governor Mervyn King indicated last night, you don't really get an accurate impression of Christmas trading until about Easter,\" said Mr Shaw. \"Our view is the Bank of England will keep its powder dry and wait to see the big picture.\"\n",
      "\n",
      "\"The retail sales figures are very weak, but as Bank of England governor Mervyn King indicated last night, you don't really get an accurate impression of Christmas trading until about Easter,\" said Mr Shaw.The last time retailers endured a tougher Christmas was 23 years previously, when sales plunged 1.7%.A number of retailers have already reported poor figures for December.Retail sales dropped by 1% on the month in December, after a 0.6% rise in November, the Office for National Statistics (ONS) said.Clothing retailers and non-specialist stores were the worst hit with only internet retailers showing any significant growth, according to the ONS.UK retail sales fell in December, failing to meet expectations and making it by some counts the worst Christmas since 1981.Yet, other retailers - including HMV, Monsoon, Jessops, Body Shop and Tesco - reported that festive sales were well up on last year.\n",
      "\n",
      "\n",
      "[18269, 444, 37, 21027, 241, 11055, 312, 248, 1863, 2288, 2507, 10267, 9262, 37, 4371, 3220, 7290, 1428, 204, 2747, 28, 193, 193, 15259, 5543, 3220, 5514, 272, 3960, 23, 14991, 271, 1679, 8072, 273, 1591, 334, 431, 596, 13995, 248, 7290, 4371, 1428, 204, 2747, 28, 25, 193, 193, 39035, 3220, 7623, 431, 204, 28, 16, 313, 248, 1325, 272, 3960, 23, 852, 241, 204, 27, 25, 33, 16, 5923, 272, 3895, 23, 248, 4966, 312, 2926, 19333, 204, 19, 32994, 20, 746, 25, 390, 7249, 62, 20640, 248, 5310, 204, 853, 31, 2804, 275, 3047, 901, 427, 248]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[18269, 444, 37, 21027, 241, 11055, 312, 248, 1863, 2288, 2507, 10267, 9262, 37, 4371, 3220, 7290, 1428, 204, 2747, 28, 193, 193, 15259, 5543, 3220, 5514, 272, 3960, 23, 14991, 271, 1679, 8072, 273, 1591, 334, 431, 596, 13995, 248, 7290, 4371, 1428, 204, 2747, 28, 25, 193, 193, 39035, 3220, 7623, 431, 204, 28, 16, 313, 248, 1325, 272, 3960, 23, 852, 241, 204, 27, 25, 33, 16, 5923, 272, 3895, 23, 248, 4966, 312, 2926, 19333, 204, 19, 32994, 20, 746, 25, 390, 7249, 62, 20640, 248, 5310, 204, 853, 31, 2804, 275, 3047, 901, 427, 248]\n",
      "\n",
      "\n",
      "2048\n",
      "2048\n"
     ]
    }
   ],
   "source": [
    "print(lm_dataset_instruction[0]['id'])\n",
    "print(lm_dataset_instruction[0]['content'])\n",
    "print(lm_dataset_instruction[0]['summary'])\n",
    "print(\"\\n\")\n",
    "print(lm_dataset_instruction[0]['input_ids'][:100])\n",
    "print(lm_dataset_instruction[0]['token_type_ids'][:100])\n",
    "print(lm_dataset_instruction[0]['attention_mask'][:100])\n",
    "print(lm_dataset_instruction[0]['labels'][:100])\n",
    "print(\"\\n\")\n",
    "print(len(lm_dataset_instruction[0]['input_ids']))\n",
    "print(len(lm_dataset_instruction[1]['input_ids']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading tokenized and chunked datasets to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 640\n",
       "})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_dataset_domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def upload_dataset_to_s3(lm_dataset, bucket, s3_prefix, dataset_name):\n",
    "\n",
    "    training_input_path = os.path.join(\"s3://\", bucket, s3_prefix, dataset_name, \"train\", \"\")\n",
    "    lm_dataset.save_to_disk(training_input_path)\n",
    "    \n",
    "    return training_input_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/640 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_input_path_domain = upload_dataset_to_s3(lm_dataset_domain, sagemaker_session_bucket, s3_prefix, \"tokenized-domain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_input_path_instruct = upload_dataset_to_s3(lm_dataset_instruction, sagemaker_session_bucket, s3_prefix, \"tokenized-instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'training_input_path_domain' (str)\n",
      "Stored 'training_input_path_instruct' (str)\n"
     ]
    }
   ],
   "source": [
    "%store test_input_path\n",
    "%store training_input_path_domain\n",
    "%store training_input_path_instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
