{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End to end fine tuning Notebook\n",
    "\n",
    "1. data preparation\n",
    "2. tokenization\n",
    "3. fine tuning with QLora\n",
    "3. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install huggingface_hub --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "awscli 1.27.150 requires PyYAML<5.5,>=3.10, but you have pyyaml 6.0.1 which is incompatible.\n",
      "docker-compose 1.29.2 requires PyYAML<6,>=3.10, but you have pyyaml 6.0.1 which is incompatible.\n",
      "jupyterlab-server 2.22.1 requires jsonschema>=4.17.3, but you have jsonschema 3.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install \"transformers==4.30.2\" \"datasets[s3]==2.13.0\" sagemaker --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are going to use Sagemaker in a local environment. You need access to an IAM Role with the required permissions for Sagemaker. You can find [here](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) more about it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#required to work in local_mode on your notebook instance for development/debugging purpose\n",
    "#!pip install 'sagemaker[local]' --upgrade --quiet\n",
    "#!pip install docker-compose --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "import os\n",
    "from sagemaker import LocalSession\n",
    "\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "#uncomment to run in local mode\n",
    "#sess = LocalSession()\n",
    "\n",
    "#the below help setting up the container's root on the EBS volume of your instance.\n",
    "#sess.config = {'local' : {'local_code' : True, 'container_root' : '/home/ec2-user/SageMaker/'}}\n",
    "\n",
    "#if you're running local mode and run into out of space issues, consider running docker_scripts/prepare-docker.sh to set the docker root under /home/ec2-user/SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::327216439222:role/Sagemaker\n",
      "sagemaker bucket: sagemaker-us-east-1-327216439222\n",
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "#region\n",
    "region = sess.boto_region_name\n",
    "\n",
    "notebook_home = \"/home/ec2-user/SageMaker/\"\n",
    "\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "s3_client = boto3.client(\"s3\")    \n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {region}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model selection\n",
    "\n",
    "Choose the model you want to fine tune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id = \"tiiuae/falcon-7b\"\n",
    "#model_id = \"tiiuae/falcon-7b-instruct\"\n",
    "model_name = model_id.split(\"/\")[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BBC Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're using BBC articles for our fine tuning contained in the local zip file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "#name fo the zip file that we'll use\n",
    "data_zip = \"BBC_news_summary.zip\"\n",
    "s3_prefix = \"model-fine-tuning\"\n",
    "\n",
    "base_dir = os.path.join(os.getcwd())\n",
    "\n",
    "path_to_file = os.path.join(os.getcwd(), \"data\", data_zip)\n",
    "\n",
    "#unziping file\n",
    "with zipfile.ZipFile(os.path.join(base_dir, \"data\", data_zip), 'r') as zip_ref:\n",
    "    zip_ref.extractall(os.path.join(notebook_home, \"data\"))\n",
    "\n",
    "#Folders that we'll iterate through after unzipping.\n",
    "articles_folder = \"News Articles\"\n",
    "summaries_folder = \"Summaries\"\n",
    "sub_folders = [\"business\", \"entertainment\", \"politics\", \"sport\", \"tech\"]\n",
    "\n",
    "articles_folders = f\"{notebook_home}/data/BBC_news_summary/\" + articles_folder\n",
    "summaries_folder = f\"{notebook_home}/data/BBC_news_summary/\" + summaries_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform folder base data into jsonlines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See below the format that we want:\n",
    "\n",
    "{\n",
    "\n",
    "  \"id\": \"13818513\",\n",
    "  \n",
    "  \"summary\": \"Amanda baked cookies and will bring Jerry some tomorrow.\",\n",
    "  \n",
    "  \"content\": \"Amanda: I baked cookies. Do you want some?\\r\\nJerry: Sure!\\r\\nAmanda: I'll bring you tomorrow :-)\"\n",
    "  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping:sport_199 due to UnicodeDecodeError\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(os.path.join(notebook_home, \"data\", \"data_jsonlines.jsonl\"), 'w') as outfile:\n",
    "    for folder in os.scandir(path = articles_folders):\n",
    "        for filename in os.scandir(path = articles_folders + \"/\" + str(folder.name)):\n",
    "            if filename.is_file():\n",
    "                try:\n",
    "                    #create article id of the form folder_001\n",
    "                    id_article = str(folder.name) + \"_\" + str(filename.name).split(\".\")[0]\n",
    "\n",
    "                    #get article content\n",
    "                    content = \"\"\n",
    "                    with open(filename, 'rb') as file:\n",
    "                        content = file.read()\n",
    "                    #get article summary\n",
    "                    summary = \"\"\n",
    "                    equivalent_summary_file = summaries_folder + \"/\" + str(folder.name) + \"/\" + str(filename.name)\n",
    "                    with open(equivalent_summary_file, 'rb') as file:\n",
    "                        summary = file.read()\n",
    "\n",
    "                    #create json object\n",
    "                    data = {}\n",
    "                    data['id'] = id_article\n",
    "                    data['content'] = content.decode(\"utf-8\")\n",
    "                    data['summary'] = summary.decode(\"utf-8\")\n",
    "   \n",
    "                    json.dump(data, outfile)\n",
    "                    outfile.write('\\n')\n",
    "\n",
    "                except UnicodeDecodeError:\n",
    "                    print(f\"skipping:{id_article} due to UnicodeDecodeError\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to /home/ec2-user/.cache/huggingface/datasets/json/default-c37d906bda4cd879/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2d51f21ae304fa7b90bb878855bc979",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c4a50d855fc46e99e71fce113133d84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /home/ec2-user/.cache/huggingface/datasets/json/default-c37d906bda4cd879/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, load_from_disk\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "#we load the data into a dataset object\n",
    "dataset = load_dataset('json', data_files=os.path.join(notebook_home, \"data\", \"data_jsonlines.jsonl\"), split=\"train\")\n",
    "\n",
    "# Load tokenizer of falcon\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.model_max_length = 2048 # overwrite wrong value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the template to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2224 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from random import randint\n",
    "from itertools import chain\n",
    "from functools import partial\n",
    "from string import Template\n",
    "\n",
    "# custom instruct prompt start\n",
    "prompt_template_domain = f\"Provide a summary for the following article:\\n{{content}}\\n---\\nSummary:\\n{{summary}}{{eos_token}}\"\n",
    "\n",
    "prompt_template_instruction = {\n",
    "\"prompt\": \"Provide a summary for the following text article:. Text: $content\",\n",
    "\"completion\": \"$summary\",\n",
    "}\n",
    "\n",
    "# template dataset to add prompt to each sample\n",
    "def template_dataset_domain_tuning(sample):\n",
    "    sample[\"text\"] = prompt_template_domain.format(content=sample[\"content\"],\n",
    "                                            summary=sample[\"summary\"],\n",
    "                                            eos_token=tokenizer.eos_token)\n",
    "    return sample\n",
    "\n",
    "def template_dataset_instruction_tuning(sample):\n",
    "    prompt_template_instruction = {\n",
    "        \"prompt\": \"Provide a summary for the following text article:. Text: $content\",\n",
    "        \"completion\": \"$summary\",\n",
    "    }\n",
    "    \n",
    "    args = {'content': sample[\"content\"], 'summary': sample[\"summary\"]}\n",
    "    sample[\"text\"] = Template(prompt_template_instruction).substitute(args)\n",
    "    return sample\n",
    "\n",
    "# apply prompt template per sample\n",
    "dataset_domain = dataset.map(template_dataset_domain_tuning, remove_columns=list(dataset.features))\n",
    "\n",
    "dataset_instruction = dataset.map(template_dataset_instruction_tuning, remove_columns=list(dataset.features))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarize the news article:\n",
      "UK firm faces Venezuelan land row\n",
      "\n",
      "Venezuelan authorities have said they will seize land owned by a British company as part of President Chavez's agrarian reform programme.\n",
      "\n",
      "Officials in Cojedes state said on Friday that farmland owned by a subsidiary of the Vestey Group would be taken and used to settle poor farmers. The government is cracking down on so-called latifundios, or large rural estates, which it says are lying idle. The Vestey Group said it had not been informed of any planned seizure.\n",
      "\n",
      "The firm, whose Agroflora subsidiary operates 13 farms in Venezuela, insisted that it had complied fully with Venezuelan law. Prosecutors in the south of the country have targeted Hato El Charcote, a beef cattle ranch owned by Agroflora. According to Reuters, they plan to seize 12,900 acres (5,200 hectares) from the 32,000 acre (13,000 hectare) farm.\n",
      "\n",
      "Officials claim that Agroflora does not possess valid documents proving its ownership of the land in question. They also allege that areas of the ranch are not being used for any form of active production. \"The legal boundaries did not match up with the actual boundaries and there is surplus,\" state prosecutor Alexis Ortiz told Reuters. \"As a consequence the government has taken action.\"\n",
      "\n",
      "Controversial reforms passed in 2001 give the government the right to take control of private property if it is declared idle or ownership cannot be traced back to the 19th Century.\n",
      "\n",
      "Critics say the powers - which President Chavez argues are needed to help the country's poorest citizens and develop the Venezuelan economy - trample all over private property rights. The Vestey Group said it had owned the land since 1920 and would co-operate fully with the authorities. But a spokesman added: \"Agroflora is absolutely confident that what it has submitted will demonstrate the legality of its title to the land.\" The company pointed out that the farm, which employs 300 workers, provides meat solely for the Venezuelan market.\n",
      "\n",
      "Last month, the government said it had identified more than 500 idle farms and had yet to consider the status of a further 40,000. The authorities said landowners whose titles were in order and whose farms were productive had \"nothing to fear\". Under President Chavez, the Venezuelan government has steadily expanded the state's involvement in the country's economy. It recently said all mining contracts involving foreign firms would be examined to ensure they provided sufficient economic benefits to the state.\n",
      "\n",
      "---\n",
      "Summary:\n",
      "The Vestey Group said it had owned the land since 1920 and would co-operate fully with the authorities.Officials in Cojedes state said on Friday that farmland owned by a subsidiary of the Vestey Group would be taken and used to settle poor farmers.Venezuelan authorities have said they will seize land owned by a British company as part of President Chavez's agrarian reform programme.Last month, the government said it had identified more than 500 idle farms and had yet to consider the status of a further 40,000.The firm, whose Agroflora subsidiary operates 13 farms in Venezuela, insisted that it had complied fully with Venezuelan law.Under President Chavez, the Venezuelan government has steadily expanded the state's involvement in the country's economy.The authorities said landowners whose titles were in order and whose farms were productive had \"nothing to fear\".The Vestey Group said it had not been informed of any planned seizure.<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "#printing an example\n",
    "print(dataset[randint(0, len(dataset))][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenating sample dataset into chunk of equal size for the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2224 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3091 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2224 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples: 804\n"
     ]
    }
   ],
   "source": [
    "# empty list to save remainder from batches to use in next batch\n",
    "remainder = {\"input_ids\": [], \"token_type_ids\": [], \"attention_mask\": []}\n",
    "\n",
    "\n",
    "def chunk(sample, chunk_length=2048):\n",
    "    # define global remainder variable to save remainder from batches to use in next batch\n",
    "    global remainder\n",
    "    # Concatenate all texts and add remainder from previous batch\n",
    "    concatenated_examples = {k: list(chain(*sample[k])) for k in sample.keys()}\n",
    "    #print(concatenated_examples.keys())\n",
    "    concatenated_examples = {k: remainder[k] + concatenated_examples[k] for k in concatenated_examples.keys()}\n",
    "    # get total number of tokens for batch\n",
    "    batch_total_length = len(concatenated_examples[list(sample.keys())[0]])\n",
    "\n",
    "    # get max number of chunks for batch\n",
    "    if batch_total_length >= chunk_length:\n",
    "        batch_chunk_length = (batch_total_length // chunk_length) * chunk_length\n",
    "\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + chunk_length] for i in range(0, batch_chunk_length, chunk_length)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    # add remainder to global variable for next batch\n",
    "    remainder = {k: concatenated_examples[k][batch_chunk_length:] for k in concatenated_examples.keys()}\n",
    "    # prepare labels\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "\n",
    "# tokenize and chunk dataset\n",
    "lm_dataset = dataset.map(\n",
    "    lambda sample: tokenizer(sample[\"text\"]), batched=True, remove_columns=list(dataset.features)\n",
    ").map(\n",
    "    partial(chunk, chunk_length=2048),\n",
    "    batched=True,\n",
    ")\n",
    "\n",
    "# Print total number of samples\n",
    "print(f\"Total number of samples: {len(lm_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uploading tokenized and chunked dataset to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/804 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training dataset uploaded to: s3://sagemaker-us-east-1-327216439222/model-fine-tuning/tokenized/train/\n"
     ]
    }
   ],
   "source": [
    "# save train_dataset to s3\n",
    "training_input_path = os.path.join(\"s3://\", sagemaker_session_bucket, s3_prefix, \"tokenized\", \"train\", \"\")\n",
    "lm_dataset.save_to_disk(training_input_path)\n",
    "\n",
    "print(f\"training dataset uploaded to: {training_input_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and upload the model to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model_id = \"EleutherAI/gpt-j-6b\"\n",
    "#model_id = \"meta-llama/Llama-2-13b\"\n",
    "model_id = \"tiiuae/falcon-7b\"\n",
    "model_name = model_id.split(\"/\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mName: huggingface-hub\n",
      "Version: 0.16.4\n",
      "Summary: Client library to download and publish models, datasets and other repos on the huggingface.co hub\n",
      "Home-page: https://github.com/huggingface/huggingface_hub\n",
      "Author: Hugging Face, Inc.\n",
      "Author-email: julien@huggingface.co\n",
      "License: Apache\n",
      "Location: /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages\n",
      "Requires: filelock, fsspec, packaging, pyyaml, requests, tqdm, typing-extensions\n",
      "Required-by: datasets, evaluate, transformers\n"
     ]
    }
   ],
   "source": [
    "!pip show huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eac94b54470c48ba9a4145f293cba74b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 12 files:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b5ec662b31e461e80facd67e899e2ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)49f25d4eb1/README.md:   0%|          | 0.00/10.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae89c5b4c8d7446494019413247a1347",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)f25d4eb1/config.json:   0%|          | 0.00/950 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fc00cf4203f4847a3e5f68485f37e88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/configuration_RW.py:   0%|          | 0.00/2.61k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "737fe303ded34d1ba3692c8dbc3e8051",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)4eb1/modelling_RW.py:   0%|          | 0.00/47.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92f4ce12f1da4b89879678d3b04392b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "659d334cc54e43c49b2117ae7ef7737e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)d4eb1/.gitattributes:   0%|          | 0.00/1.48k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31bc3daeb0dc430481631043b014dede",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)model.bin.index.json:   0%|          | 0.00/16.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87539edb6ff84210a4ed5c105e0b3708",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/281 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ec88f23257c420c82a5751b870e56a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00002-of-00002.bin:   0%|          | 0.00/4.48G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfb13ee5c2af43e29a148f0fc832bd84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/220 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05623a8be7a24d23a99f4ebe76974079",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00001-of-00002.bin:   0%|          | 0.00/9.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b44ec12f225e40358f2a3e947c4c3fd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)d4eb1/tokenizer.json:   0%|          | 0.00/2.73M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'/home/ec2-user/SageMaker/models/falcon-7b'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "model_tar_dir = Path(os.path.join(notebook_home, \"models\", model_name))\n",
    "if not os.path.isdir(model_tar_dir):\n",
    "    os.makedirs(model_tar_dir)\n",
    "\n",
    "# Download model from Hugging Face into model_dir\n",
    "snapshot_download(model_id, \n",
    "                  local_dir=str(model_tar_dir), \n",
    "                  local_dir_use_symlinks=False,\n",
    "                  cache_dir=\"/home/ec2-user/SageMaker/models/tmp\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/models/falcon-7b\n",
      "/home/ec2-user/SageMaker/models/falcon-7b\n"
     ]
    }
   ],
   "source": [
    "print(model_tar_dir)\n",
    "print(\"/home/ec2-user/SageMaker/models/falcon-7b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cwd = str(Path.cwd())\n",
    "p = Path(os.path.join(Path.cwd(), model_tar_dir))\n",
    "mydirs = list(p.glob('**'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#uploading the model to S3\n",
    "def upload_to_s3(model_tar_dir, s3_prefix, sagemaker_session_bucket):\n",
    "    stop_list = ['.ipynb_checkpoints', '.gitattributes']\n",
    "    files = os.listdir(model_tar_dir)   \n",
    "    for file in files:\n",
    "        if file not in stop_list:\n",
    "            try:\n",
    "                local_path = os.path.join(model_tar_dir, file)\n",
    "                if os.path.isfile(local_path):\n",
    "                    remote_path = os.path.join(s3_prefix, file)\n",
    "                    s3_client.upload_file(local_path, sagemaker_session_bucket, remote_path)\n",
    "                    print(f\"{local_path} uploaded to s3 folder: {remote_path}\")\n",
    "                else:\n",
    "                    new_local_dir = os.path.join(model_tar_dir,file)\n",
    "                    new_remote_dir = os.path.join(s3_prefix,file)\n",
    "                    upload_to_s3(new_local_dir, new_remote_dir, sagemaker_session_bucket)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/models/falcon-7b/pytorch_model-00001-of-00002.bin uploaded to s3 folder: model-fine-tuning/models/falcon-7b/pytorch_model-00001-of-00002.bin\n",
      "/home/ec2-user/SageMaker/models/falcon-7b/pytorch_model.bin.index.json uploaded to s3 folder: model-fine-tuning/models/falcon-7b/pytorch_model.bin.index.json\n",
      "/home/ec2-user/SageMaker/models/falcon-7b/pytorch_model-00002-of-00002.bin uploaded to s3 folder: model-fine-tuning/models/falcon-7b/pytorch_model-00002-of-00002.bin\n",
      "/home/ec2-user/SageMaker/models/falcon-7b/test/test_file.txt uploaded to s3 folder: model-fine-tuning/models/falcon-7b/test/test_file.txt\n",
      "/home/ec2-user/SageMaker/models/falcon-7b/config.json uploaded to s3 folder: model-fine-tuning/models/falcon-7b/config.json\n",
      "/home/ec2-user/SageMaker/models/falcon-7b/README.md uploaded to s3 folder: model-fine-tuning/models/falcon-7b/README.md\n",
      "/home/ec2-user/SageMaker/models/falcon-7b/configuration_RW.py uploaded to s3 folder: model-fine-tuning/models/falcon-7b/configuration_RW.py\n",
      "/home/ec2-user/SageMaker/models/falcon-7b/modelling_RW.py uploaded to s3 folder: model-fine-tuning/models/falcon-7b/modelling_RW.py\n",
      "/home/ec2-user/SageMaker/models/falcon-7b/special_tokens_map.json uploaded to s3 folder: model-fine-tuning/models/falcon-7b/special_tokens_map.json\n",
      "/home/ec2-user/SageMaker/models/falcon-7b/tokenizer.json uploaded to s3 folder: model-fine-tuning/models/falcon-7b/tokenizer.json\n",
      "/home/ec2-user/SageMaker/models/falcon-7b/generation_config.json uploaded to s3 folder: model-fine-tuning/models/falcon-7b/generation_config.json\n",
      "/home/ec2-user/SageMaker/models/falcon-7b/tokenizer_config.json uploaded to s3 folder: model-fine-tuning/models/falcon-7b/tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "upload_to_s3(model_tar_dir, os.path.join(s3_prefix, \"models\", model_name, ''), sagemaker_session_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#storing model path and output model path to reuse later\n",
    "model_path = os.path.join(\"s3://\", sagemaker_session_bucket, s3_prefix, \"models\", model_name, '')\n",
    "fine_tuned_model_path = os.path.join(\"s3://\", sagemaker_session_bucket, s3_prefix, \"tuned-models\", model_name, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-327216439222/model-fine-tuning/tokenized/train/\n",
      "s3://sagemaker-us-east-1-327216439222/model-fine-tuning/models/falcon-7b/\n",
      "s3://sagemaker-us-east-1-327216439222/model-fine-tuning/tuned-models/falcon-7b/\n"
     ]
    }
   ],
   "source": [
    "print(training_input_path)\n",
    "print(model_path)\n",
    "print(fine_tuned_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#TO REMOVE, JUST SHORCUTING CODE\n",
    "training_input_path = \"s3://sagemaker-us-east-1-327216439222/model-fine-tuning/tokenized/train/\"\n",
    "model_path = \"s3://sagemaker-us-east-1-327216439222/model-fine-tuning/models/falcon-7b/\"\n",
    "fine_tuned_model_path = \"s3://sagemaker-us-east-1-327216439222/model-fine-tuning/tuned-models/falcon-7b/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tune with QLoRA on Amazon SageMaker\n",
    "\n",
    "We are going to use the recently introduced method in the paper \"[QLoRA: Quantization-aware Low-Rank Adapter Tuning for Language Generation](https://arxiv.org/abs/2106.09685)\" by Tim Dettmers et al. QLoRA is a new technique to reduce the memory footprint of large language models during finetuning, without sacrificing performance. The TL;DR; of how QLoRA works is: \n",
    "\n",
    "* Quantize the pretrained model to 4 bits and freezing it.\n",
    "* Attach small, trainable adapter layers. (LoRA)\n",
    "* Finetune only the adapter layers, while using the frozen quantized model for context.\n",
    "\n",
    "We prepared a [run_clm.py](./scripts/run_clm.py), which implements QLora using PEFT to train our model. The script also merges the LoRA weights into the model weights after training. That way you can use the model as a normal model without any additional code.\n",
    "\n",
    "In order to create a sagemaker training job we need an `HuggingFace` Estimator. The Estimator handles end-to-end Amazon SageMaker training and deployment tasks. The Estimator manages the infrastructure use. \n",
    "SagMaker takes care of starting and managing all the required ec2 instances for us, provides the correct huggingface container, uploads the provided scripts and downloads the data from our S3 bucket into the container at `/opt/ml/input/data`. Then, it starts the training job by running.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model to be fine-tuned: tiiuae/falcon-7b\n"
     ]
    }
   ],
   "source": [
    "print(f\"model to be fine-tuned: {model_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# create the Estimator for fine tuning on multiple GPU (and local mode)\\nhuggingface_estimator = HuggingFace(\\n    sagemaker_session=sess,                   #required for setting new container root to EBS volume\\n    entry_point          = \\'launch_accelerate.sh\\',         # train script\\n    source_dir           = \\'./qlora_scripts\\',    # directory which includes all the files needed for training\\n    instance_type        = \\'local_gpu\\',\\n    #instance_type        = \\'ml.g5.12xlarge\\',     # instances type used for the training job\\n    instance_count       = 1,                 # the number of instances used for training\\n    base_job_name        = job_name,          # the name of the training job\\n    role                 = role,              # Iam role used in training job to access AWS ressources, e.g. S3\\n    volume_size          = 400,               # the size of the EBS volume in GB\\n    transformers_version = \\'4.28.1\\',\\n    #framework_version    = \\'2.0.0\\',            # the transformers version used in the training job\\n    pytorch_version      = \\'2.0.0\\',            # the pytorch_version version used in the training job\\n    py_version           = \\'py310\\',            # the python version used in the training job\\n    hyperparameters      =  hyperparameters,\\n    environment          = { \"HUGGINGFACE_HUB_CACHE\": \"/tmp/.cache\" }, # set env variable to cache models in /tmp\\n    #this configuration enabled data/model parallel distribution\\n    #distribution={\\n    #    \"torch_distributed\": {\\n    #        \"enabled\": True\\n    #    }\\n    #}\\n)\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "# define Training Job Name \n",
    "job_name = f'huggingface-qlora-{time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())}'\n",
    "\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters ={\n",
    "  'model_id': model_id,                                # pre-trained model\n",
    "  'dataset_path': '/opt/ml/input/data/training',       # path where sagemaker will save training dataset\n",
    "  'model_path' : '/opt/ml/input/data/pre-trained/',    # path to load the model from\n",
    "  'epochs': 1,                                         # number of training epochs\n",
    "  'per_device_train_batch_size': 4,                    # batch size for training\n",
    "  'lr': 2e-4,                                          # learning rate used during training\n",
    "}\n",
    "\n",
    "\n",
    "# create the Estimator for fine tuning on single GPU\n",
    "huggingface_estimator = HuggingFace(\n",
    "    #sagemaker_session=sess,                   #required for setting new container root to EBS volume\n",
    "    entry_point          = 'run_clm.py',         # train script\n",
    "    source_dir           = './qlora_scripts',    # directory which includes all the files needed for training\n",
    "    #instance_type        = 'local_gpu',         #uncomment and comment below to switch to local mode.\n",
    "    instance_type        = 'ml.g5.12xlarge',     # instances type used for the training job\n",
    "    instance_count       = 1,                 # the number of instances used for training\n",
    "    base_job_name        = job_name,          # the name of the training job\n",
    "    role                 = role,              # Iam role used in training job to access AWS ressources, e.g. S3\n",
    "    volume_size          = 500,               # the size of the EBS volume in GB\n",
    "    transformers_version = '4.28.1',\n",
    "    #framework_version    = '2.0.0',            # the transformers version used in the training job\n",
    "    pytorch_version      = '2.0.0',            # the pytorch_version version used in the training job\n",
    "    py_version           = 'py310',            # the python version used in the training job\n",
    "    hyperparameters      =  hyperparameters,\n",
    "    environment          = { \"HUGGINGFACE_HUB_CACHE\": \"/tmp/.cache\" }, # set env variable to cache models in /tmp\n",
    ")\n",
    "\n",
    "'''\n",
    "# create the Estimator for fine tuning on multiple GPU (and local mode)\n",
    "huggingface_estimator = HuggingFace(\n",
    "    sagemaker_session=sess,                   #required for setting new container root to EBS volume\n",
    "    entry_point          = 'launch_accelerate.sh',         # train script\n",
    "    source_dir           = './qlora_scripts',    # directory which includes all the files needed for training\n",
    "    instance_type        = 'local_gpu',\n",
    "    #instance_type        = 'ml.g5.12xlarge',     # instances type used for the training job\n",
    "    instance_count       = 1,                 # the number of instances used for training\n",
    "    base_job_name        = job_name,          # the name of the training job\n",
    "    role                 = role,              # Iam role used in training job to access AWS ressources, e.g. S3\n",
    "    volume_size          = 400,               # the size of the EBS volume in GB\n",
    "    transformers_version = '4.28.1',\n",
    "    #framework_version    = '2.0.0',            # the transformers version used in the training job\n",
    "    pytorch_version      = '2.0.0',            # the pytorch_version version used in the training job\n",
    "    py_version           = 'py310',            # the python version used in the training job\n",
    "    hyperparameters      =  hyperparameters,\n",
    "    environment          = { \"HUGGINGFACE_HUB_CACHE\": \"/tmp/.cache\" }, # set env variable to cache models in /tmp\n",
    "    #this configuration enabled data/model parallel distribution\n",
    "    #distribution={\n",
    "    #    \"torch_distributed\": {\n",
    "    #        \"enabled\": True\n",
    "    #    }\n",
    "    #}\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now start our training job, with the `.fit()` method passing our S3 path to the training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: huggingface-qlora-2023-07-26-00-26-18-2023-07-26-00-26-23-935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.inputs import TrainingInput\n",
    "#from torch import inf\n",
    "\n",
    "fast_file = lambda x: TrainingInput(x, input_mode=\"FastFile\")\n",
    "huggingface_estimator.fit(\n",
    "    {\n",
    "        \"pre-trained\": fast_file(model_path),\n",
    "        \"training\": fast_file(training_input_path),\n",
    "    },\n",
    "    wait=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'huggingface-qlora-2023-07-25-07-31-59-2023-07-25-07-32-09-436'"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_job_name = huggingface_estimator.latest_training_job.name\n",
    "training_job_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our example, the SageMaker training job took 1h for one epoch. The ml.g5.12xlarge instance we used costs `$7.09 per hour` for on-demand usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-327216439222/huggingface-qlora-2023-07-25-07-31-59-2023-07-25-07-32-09-436/output/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "#url to the fined tuned model\n",
    "model_tar_gz_s3 = os.path.join(\"s3://\", sagemaker_session_bucket, training_job_name, \"output/model.tar.gz\")\n",
    "print(model_tar_gz_s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait for the model to be fine tuned and model.tar.gz to be created or use one already created from folder: < TODO provide folder >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading the fine tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/models/falcon-7b-tuned/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "model_tuned_dir = os.path.join(notebook_home, \"models\", model_name + \"-tuned\", \"\")\n",
    "if not os.path.isdir(model_tuned_dir):\n",
    "    os.makedirs(model_tuned_dir)\n",
    "model_tuned_file = model_tuned_dir + \"model.tar.gz\"\n",
    "print(model_tuned_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#download from s3 to local\n",
    "s3_client.download_file(sagemaker_session_bucket, \n",
    "                        os.path.join(training_job_name, \"output/model.tar.gz\"), \n",
    "                       model_tuned_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#untar to check the content and make sure it includes everything\n",
    "!tar -xvf $model_tuned_file --directory $model_tuned_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model group already exists\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "sm_client = boto3.client('sagemaker', region_name=region)\n",
    "\n",
    "model_package_group_name = \"gai-fine-tuned-\" + model_name\n",
    "\n",
    "try:\n",
    "    model_package_group_input_dict = {\n",
    "     \"ModelPackageGroupName\" : model_package_group_name,\n",
    "     \"ModelPackageGroupDescription\" : f\"fine tuned versions of {model_id}\"\n",
    "    }\n",
    "    create_model_package_group_response = sm_client.create_model_package_group(**model_package_group_input_dict)\n",
    "    print('ModelPackageGroup Arn : {}'.format(create_model_package_group_response['ModelPackageGroupArn']))\n",
    "except:\n",
    "    print(\"Model group already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register a model version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ModelPackageGroupName': 'gai-fine-tuned-falcon-7b',\n",
       " 'ModelPackageDescription': 'qlora fine tuning of model huggingface-llm-falcon-7b-bf16 done during training job huggingface-qlora-2023-07-24-11-45-33-2023-07-24-11-45-36-488',\n",
       " 'ModelApprovalStatus': 'PendingManualApproval',\n",
       " 'InferenceSpecification': {'Containers': [{'Image': '763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-inference:2.0.0-transformers4.28.1-gpu-py310-cu118-ubuntu20.04',\n",
       "    'ModelDataUrl': 's3://sagemaker-us-east-1-327216439222/huggingface-qlora-2023-07-24-11-45-33-2023-07-24-11-45-36-488/output/model.tar.gz'}],\n",
       "  'SupportedContentTypes': ['application/json'],\n",
       "  'SupportedResponseMIMETypes': ['application/json']}}"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_url = \"763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-inference:2.0.0-transformers4.28.1-gpu-py310-cu118-ubuntu20.04\"\n",
    "\n",
    "modelpackage_inference_specification =  {\n",
    "    \"InferenceSpecification\": {\n",
    "      \"Containers\": [\n",
    "         {\n",
    "            \"Image\": image_url,\n",
    "             \"ModelDataUrl\": model_tar_gz_s3\n",
    "         }\n",
    "      ],\n",
    "      \"SupportedContentTypes\": [\"application/json\"],\n",
    "      \"SupportedResponseMIMETypes\": [\"application/json\"],\n",
    "   }\n",
    " }\n",
    "\n",
    "create_model_package_input_dict = {\n",
    "    \"ModelPackageGroupName\" : model_package_group_name,\n",
    "    \"ModelPackageDescription\" : f\"qlora fine tuning of model {model_id} done during training job {training_job_name}\",\n",
    "    \"ModelApprovalStatus\" : \"PendingManualApproval\"\n",
    "}\n",
    "\n",
    "create_model_package_input_dict.update(modelpackage_inference_specification)\n",
    "\n",
    "create_model_package_input_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelPackage Version ARN : arn:aws:sagemaker:us-east-1:327216439222:model-package/gai-fine-tuned-falcon-7b/1\n"
     ]
    }
   ],
   "source": [
    "create_model_package_response = sm_client.create_model_package(**create_model_package_input_dict)\n",
    "model_package_arn = create_model_package_response[\"ModelPackageArn\"]\n",
    "print('ModelPackage Version ARN : {}'.format(model_package_arn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ModelPackageSummaryList': [{'ModelPackageGroupName': 'gai-fine-tuned-falcon-7b',\n",
       "   'ModelPackageVersion': 1,\n",
       "   'ModelPackageArn': 'arn:aws:sagemaker:us-east-1:327216439222:model-package/gai-fine-tuned-falcon-7b/1',\n",
       "   'ModelPackageDescription': 'qlora fine tuning of model huggingface-llm-falcon-7b-bf16 done during training job huggingface-qlora-2023-07-24-11-45-33-2023-07-24-11-45-36-488',\n",
       "   'CreationTime': datetime.datetime(2023, 7, 25, 0, 18, 49, 264000, tzinfo=tzlocal()),\n",
       "   'ModelPackageStatus': 'Completed',\n",
       "   'ModelApprovalStatus': 'PendingManualApproval'}],\n",
       " 'ResponseMetadata': {'RequestId': 'cb1f1c2e-d548-4620-a38d-0b888e929507',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'cb1f1c2e-d548-4620-a38d-0b888e929507',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '492',\n",
       "   'date': 'Tue, 25 Jul 2023 00:19:01 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_client.list_model_packages(ModelPackageGroupName=model_package_group_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approve model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_package_update_input_dict = {\n",
    "    \"ModelPackageArn\" : model_package_arn,\n",
    "    \"ModelApprovalStatus\" : \"Approved\"\n",
    "}\n",
    "model_package_update_response = sm_client.update_model_package(**model_package_update_input_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the fined tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'falcon-7b-tuned-2023-07-25-00-27-41'"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoint_name = f'{model_name}-tuned-{time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())}'\n",
    "endpoint_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### from the registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker import ModelPackage\n",
    "from time import gmtime, strftime\n",
    "\n",
    "def deploy_from_registry(role, model_package_arn, sess, name):\n",
    "    model = ModelPackage(role=role, \n",
    "                         model_package_arn=model_package_arn, \n",
    "                         sagemaker_session=sess)\n",
    "    model.deploy(initial_instance_count=1, instance_type='ml.g5.12xlarge', wait=False, endpoint_name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: gai-fine-tuned-falcon-7b-2023-07-25-00-26-10-810\n",
      "INFO:sagemaker:Creating endpoint-config with name falcon-7b-tuned-2023-07-25-00-26-09\n",
      "INFO:sagemaker:Creating endpoint with name falcon-7b-tuned-2023-07-25-00-26-09\n"
     ]
    }
   ],
   "source": [
    "deploy_from_registry(role, model_package_arn, sess, endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### or directly with HuggingFaceModel sagemaker API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-1-327216439222/huggingface-qlora-2023-07-24-11-45-33-2023-07-24-11-45-36-488/output/model.tar.gz'"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_tar_gz_s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "\n",
    "#URL: 763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-inference:2.0.0-transformers4.28.1-gpu-py310-cu118-ubuntu20.04\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "   model_data=model_tar_gz_s3,\n",
    "   role=role, \n",
    "   transformers_version=\"4.28.1\", \n",
    "   pytorch_version=\"2.0.0\", \n",
    "   py_version=\"py310\",\n",
    "   model_server_workers=1,\n",
    ")\n",
    "\n",
    "# deploy model to SageMaker Inference\n",
    "predictor_hf = huggingface_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type= \"ml.g5.12xlarge\",\n",
    "    wait=False,\n",
    "    endpoint_name=endpoint_name,\n",
    ")\n",
    "\n",
    "endpoint_name = predictor_hf.endpoint_name\n",
    "print(endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the original model for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: hf-llm-falcon-7b-bf16-2023-07-26-00-51-19-279\n",
      "INFO:sagemaker:Creating endpoint-config with name hf-llm-falcon-7b-bf16-2023-07-26-00-51-19-603\n",
      "INFO:sagemaker:Creating endpoint with name hf-llm-falcon-7b-bf16-2023-07-26-00-51-19-603\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "\n",
    "model_id, model_version = \"huggingface-llm-falcon-7b-bf16\", \"*\"\n",
    "\n",
    "js_model = JumpStartModel(model_id=model_id, instance_type=\"ml.g5.12xlarge\")\n",
    "predictor_js = js_model.deploy(wait=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Querying the endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#setting it manually here\n",
    "endpoint_name = \"falcon-7b-tuned-2023-07-25-00-26-09\"\n",
    "#endpoint_name = \"gai-fine-tuned-falcon-7b-1690174749-2023-07-24-06-42-41-602\"\n",
    "original_model_endpoint_name = \"hf-llm-falcon-7b-bf16-2023-07-24-12-14-32-008\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def query_endpoint_with_json_payload(encoded_json, endpoint_name, content_type=\"application/json\"):\n",
    "    client = boto3.client(\"runtime.sagemaker\")\n",
    "    response = client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name, ContentType=content_type, Body=encoded_json\n",
    "    )\n",
    "    return response\n",
    "\n",
    "#method used to parse the inference model's response. we pass it as part of the model's config\n",
    "def parse_response_model(query_response):\n",
    "    model_predictions = json.loads(query_response[\"Body\"].read())\n",
    "    return [gen[\"generated_text\"] for gen in model_predictions]\n",
    "\n",
    "def query_llm(payload, endpoint_name):\n",
    "    query_response = query_endpoint_with_json_payload(json.dumps(payload).encode(\"utf-8\"), endpoint_name=endpoint_name)\n",
    "    return parse_response_model(query_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_template = f\"Summarize the following text:\\n{{text}}\\n---\\nSummary:\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = \"\"\"Ad sales boost Time Warner profit. Quarterly profits at US media giant TimeWarner jumped 76% to $1.13bn (£600m) for the three months to December, from $639m year-earlier.The firm, which is now one of the biggest investors in Google, benefited from sales of high-speed internet connections and higher advert sales. TimeWarner said fourth quarter sales rose 2% to $11.1bn from $10.9bn. Its profits were buoyed by one-off gains which offset a profit dip at Warner Bros, and less users for AOL.Time Warner said on Friday that it now owns 8% of search-engine Google. But its own internet business, AOL, had has mixed fortunes. It lost 464,000 subscribers in the fourth quarter profits were lower than in the preceding three quarters. However, the company said AOL's underlying profit before exceptional items rose 8% on the back of stronger internet advertising revenues. It hopes to increase subscribers by offering the online service free to TimeWarner internet customers and will try to sign up AOL's existing customers for high-speed broadband. TimeWarner also has to restate 2000 and 2003 results following a probe by the US Securities Exchange Commission (SEC), which is close to concluding.Time Warner's fourth quarter profits were slightly better than analysts' expectations. But its film division saw profits slump 27% to $284m, helped by box-office flops Alexander and Catwoman, a sharp contrast to year-earlier, when the third and final film in the Lord of the Rings trilogy boosted results. For the full-year, TimeWarner posted a profit of $3.36bn, up 27% from its 2003 performance, while revenues grew 6.4% to $42.09bn. \"Our financial performance was strong, meeting or exceeding all of our full-year objectives and greatly enhancing our flexibility,\" chairman and chief executive Richard Parsons said. For 2005, TimeWarner is projecting operating earnings growth of around 5%, and also expects higher revenue and wider profit margins. TimeWarner is to restate its accounts as part of efforts to resolve an inquiry into AOL by US market regulators. It has already offered to pay $300m to settle charges, in a deal that is under review by the SEC. The company said it was unable to estimate the amount it needed to set aside for legal reserves, which it previously set at $500m. It intends to adjust the way it accounts for a deal with German music publisher Bertelsmann's purchase of a stake in AOL Europe, which it had reported as advertising revenue. It will now book the sale of its stake in AOL Europe as a loss on the value of that stake.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = prompt_template.format(text=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "payload = {\n",
    "    \"inputs\": prompt,\n",
    "    \"parameters\":{\n",
    "        \"do_sample\": True,\n",
    "        \"top_p\": 0.9,\n",
    "        \"temperature\": 0.8,\n",
    "        \"max_new_tokens\": 1024,\n",
    "        \"stop\": [\"<|endoftext|>\", \"</s>\"]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarize the following text:\n",
      "Ad sales boost Time Warner profit. Quarterly profits at US media giant TimeWarner jumped 76% to $1.13bn (£600m) for the three months to December, from $639m year-earlier.The firm, which is now one of the biggest investors in Google, benefited from sales of high-speed internet connections and higher advert sales. TimeWarner said fourth quarter sales rose 2% to $11.1bn from $10.9bn. Its profits were buoyed by one-off gains which offset a profit dip at Warner Bros, and less users for AOL.Time Warner said on Friday that it now owns 8% of search-engine Google. But its own internet business, AOL, had has mixed fortunes. It lost 464,000 subscribers in the fourth quarter profits were lower than in the preceding three quarters. However, the company said AOL's underlying profit before exceptional items rose 8% on the back of stronger internet advertising revenues. It hopes to increase subscribers by offering the online service free to TimeWarner internet customers and will try to sign up AOL's existing customers for high-speed broadband. TimeWarner also has to restate 2000 and 2003 results following a probe by the US Securities Exchange Commission (SEC), which is close to concluding.Time Warner's fourth quarter profits were slightly better than analysts' expectations. But its film division saw profits slump 27% to $284m, helped by box-office flops Alexander and Catwoman, a sharp contrast to year-earlier, when the third and final film in the Lord of the Rings trilogy boosted results. For the full-year, TimeWarner posted a profit of $3.36bn, up 27% from its 2003 performance, while revenues grew 6.4% to $42.09bn. \"Our financial performance was strong, meeting or exceeding all of our full-year objectives and greatly enhancing our flexibility,\" chairman and chief executive Richard Parsons said. For 2005, TimeWarner is projecting operating earnings growth of around 5%, and also expects higher revenue and wider profit margins. TimeWarner is to restate its accounts as part of efforts to resolve an inquiry into AOL by US market regulators. It has already offered to pay $300m to settle charges, in a deal that is under review by the SEC. The company said it was unable to estimate the amount it needed to set aside for legal reserves, which it previously set at $500m. It intends to adjust the way it accounts for a deal with German music publisher Bertelsmann's purchase of a stake in AOL Europe, which it had reported as advertising revenue. It will now book the sale of its stake in AOL Europe as a loss on the value of that stake.\n",
      "---\n",
      "Summary:\n",
      "\n",
      "original:[\"Time Warner reported that it's fourth-quarter earnings were up 76 percent, while it's full year profits were up 27 percent from last year. Time Warner has been helped by the success of the Lord of the Rings trilogy, and has been bolstered by its investment in Google, which owns 8 percent of Google. Time Warner's sales in the fourth quarter were down slightly due to a decline in its film division.\\nThe company is one of the biggest investors in Google, and has been a pioneer in using the Internet for its business. Time Warner also owns the popular AOL, and is trying to get more subscribers by offering the service for free.\"]\n"
     ]
    }
   ],
   "source": [
    "print(prompt)\n",
    "print(f\"original:{query_llm(payload, original_model_endpoint_name)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarize the following text:\n",
      "Ad sales boost Time Warner profit. Quarterly profits at US media giant TimeWarner jumped 76% to $1.13bn (£600m) for the three months to December, from $639m year-earlier.The firm, which is now one of the biggest investors in Google, benefited from sales of high-speed internet connections and higher advert sales. TimeWarner said fourth quarter sales rose 2% to $11.1bn from $10.9bn. Its profits were buoyed by one-off gains which offset a profit dip at Warner Bros, and less users for AOL.Time Warner said on Friday that it now owns 8% of search-engine Google. But its own internet business, AOL, had has mixed fortunes. It lost 464,000 subscribers in the fourth quarter profits were lower than in the preceding three quarters. However, the company said AOL's underlying profit before exceptional items rose 8% on the back of stronger internet advertising revenues. It hopes to increase subscribers by offering the online service free to TimeWarner internet customers and will try to sign up AOL's existing customers for high-speed broadband. TimeWarner also has to restate 2000 and 2003 results following a probe by the US Securities Exchange Commission (SEC), which is close to concluding.Time Warner's fourth quarter profits were slightly better than analysts' expectations. But its film division saw profits slump 27% to $284m, helped by box-office flops Alexander and Catwoman, a sharp contrast to year-earlier, when the third and final film in the Lord of the Rings trilogy boosted results. For the full-year, TimeWarner posted a profit of $3.36bn, up 27% from its 2003 performance, while revenues grew 6.4% to $42.09bn. \"Our financial performance was strong, meeting or exceeding all of our full-year objectives and greatly enhancing our flexibility,\" chairman and chief executive Richard Parsons said. For 2005, TimeWarner is projecting operating earnings growth of around 5%, and also expects higher revenue and wider profit margins. TimeWarner is to restate its accounts as part of efforts to resolve an inquiry into AOL by US market regulators. It has already offered to pay $300m to settle charges, in a deal that is under review by the SEC. The company said it was unable to estimate the amount it needed to set aside for legal reserves, which it previously set at $500m. It intends to adjust the way it accounts for a deal with German music publisher Bertelsmann's purchase of a stake in AOL Europe, which it had reported as advertising revenue. It will now book the sale of its stake in AOL Europe as a loss on the value of that stake.\n",
      "---\n",
      "Summary:\n",
      "\n"
     ]
    },
    {
     "ename": "ModelError",
     "evalue": "An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (400) from primary with message \"{\n  \"code\": 400,\n  \"type\": \"InternalServerException\",\n  \"message\": \"module \\u0027transformers_modules.model.modelling_RW\\u0027 has no attribute \\u0027RWForCausalLM\\u0027\"\n}\n\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/falcon-7b-tuned-2023-07-25-00-26-09 in account 327216439222 for more information.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModelError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[242], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(prompt)\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfine-tuned:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mquery_llm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[43mendpoint_name\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[231], line 14\u001b[0m, in \u001b[0;36mquery_llm\u001b[0;34m(prompt_template, endpoint_name)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mquery_llm\u001b[39m(prompt_template, endpoint_name):\n\u001b[0;32m---> 14\u001b[0m     query_response \u001b[38;5;241m=\u001b[39m \u001b[43mquery_endpoint_with_json_payload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendpoint_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parse_response_model(query_response)\n",
      "Cell \u001b[0;32mIn[231], line 3\u001b[0m, in \u001b[0;36mquery_endpoint_with_json_payload\u001b[0;34m(encoded_json, endpoint_name, content_type)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mquery_endpoint_with_json_payload\u001b[39m(encoded_json, endpoint_name, content_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapplication/json\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m      2\u001b[0m     client \u001b[38;5;241m=\u001b[39m boto3\u001b[38;5;241m.\u001b[39mclient(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mruntime.sagemaker\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke_endpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mEndpointName\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mContentType\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontent_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoded_json\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/botocore/client.py:530\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    526\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    527\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpy_operation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m() only accepts keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    528\u001b[0m     )\n\u001b[1;32m    529\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 530\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/botocore/client.py:964\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    962\u001b[0m     error_code \u001b[38;5;241m=\u001b[39m parsed_response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCode\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    963\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m--> 964\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m    965\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    966\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parsed_response\n",
      "\u001b[0;31mModelError\u001b[0m: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (400) from primary with message \"{\n  \"code\": 400,\n  \"type\": \"InternalServerException\",\n  \"message\": \"module \\u0027transformers_modules.model.modelling_RW\\u0027 has no attribute \\u0027RWForCausalLM\\u0027\"\n}\n\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/falcon-7b-tuned-2023-07-25-00-26-09 in account 327216439222 for more information."
     ]
    }
   ],
   "source": [
    "print(prompt)\n",
    "print(f\"fine-tuned:{query_llm(payload, endpoint_name)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of the fine tuned model\n",
    "\n",
    "We're using the LM Evaluation Harness framework to evaluate our fine tuned model\n",
    "\n",
    "https://github.com/EleutherAI/lm-evaluation-harness\n",
    "\n",
    "list of possible test/tasks to use:\n",
    "\n",
    "https://github.com/EleutherAI/lm-evaluation-harness/blob/master/docs/task_table.md\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### installation of lm-evaluation-harness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Cloning into 'lm-evaluation-harness'...\n",
      "remote: Enumerating objects: 14128, done.\u001b[K\n",
      "remote: Counting objects: 100% (2540/2540), done.\u001b[K\n",
      "remote: Compressing objects: 100% (483/483), done.\u001b[K\n",
      "remote: Total 14128 (delta 2264), reused 2158 (delta 2053), pack-reused 11588\u001b[K\n",
      "Receiving objects: 100% (14128/14128), 19.21 MiB | 26.62 MiB/s, done.\n",
      "Resolving deltas: 100% (9379/9379), done.\n",
      "branch 'big-refactor' set up to track 'origin/big-refactor'.\n",
      "Switched to a new branch 'big-refactor'\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: Ignoring invalid distribution -orch (/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#installing the latest version of lm-evaluation-harness. note that the big-refactor is the new version branch that will probably become the main one soon.\n",
    "#tested on revision 2820042d05e91c87852c82293f8973dc841c1a25 of the big-refactor branch\n",
    "!git clone https://github.com/EleutherAI/lm-evaluation-harness && cd lm-evaluation-harness && git checkout big-refactor && pip install -e . --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you need to modify lm-evaluation-harness/lm_eval/models/huggingface.py and add around line 60:\n",
    "\n",
    "MODEL_FOR_CAUSAL_LM_MAPPING_NAMES[\"RefinedWebModel\"] = \"RWForCausalLM\"\n",
    "\n",
    "otherwise \"self._config = transformers.AutoConfig.from_pretrained\" set the model_type to \"RefinedWebModel\" which is not in the mapping list for MODEL_FOR_CAUSAL_LM_MAPPING_NAMES.\n",
    "\n",
    "as a result the auto_model_class selected  is transformers.AutoModelForSeq2SeqLM instead of transformers.AutoModelForCausalLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which tasks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not all tasks are available in the new refactored lm-evaluation-harness framework (you can always use the old version). see below for the WIP list:\n",
    "\n",
    "https://github.com/EleutherAI/lm-evaluation-harness/tree/big-refactor/lm_eval/tasks\n",
    "\n",
    "you'll find details under each folder for the tasks. the challenge is that there are 200+ tasks from the old frameworks so it is challenging to know which ones is the best of what you're trying to evaluate.\n",
    "\n",
    "See below some resources that I found that can help:\n",
    "\n",
    "https://super.gluebenchmark.com/tasks\n",
    "\n",
    "https://gluebenchmark.com/tasks/\n",
    "\n",
    "Notes from tasks that have already moved to the new framework:\n",
    "\n",
    "- anli: Q&A\n",
    "- ARC: Q&A science exam\n",
    "- arithmetic\n",
    "- hellaswag: sentence finishing\n",
    "- lambada: next word prediction\n",
    "- glue/qnli: Q&A https://rajpurkar.github.io/SQuAD-explorer/\n",
    "- gsm8k: math problems\n",
    "- headqa: health care complex reasoning\n",
    "- hendrycks_ethics: ethic benchmark\n",
    "- mathqa: math word problem solving\n",
    "- openbookqa: Q&A??\n",
    "- pile: general comprehension/Q&A across books, github repositories, webpages, chat logs, and medical, physics, math, computer science, and philosophy papers.\n",
    "- piqa: commonsense Q&A, https://huggingface.co/datasets/piqa\n",
    "- prost: contains 18,736 multiple-choice questions made from 14 manually curated templates, covering 10 physical reasoning concepts, https://paperswithcode.com/dataset/prost\n",
    "- pubmedqa: biomedical Q&A https://github.com/pubmedqa/pubmedqa\n",
    "- qa4mre: Q&A https://www.tensorflow.org/datasets/catalog/qa4mre\n",
    "- race Q&A/comprehension https://huggingface.co/datasets/race\n",
    "- sciq: crowdsourced science exam questions about Physics, Chemistry and Biology, https://huggingface.co/datasets/sciq\n",
    "- webqs: WebQuestions is a benchmark for question answering.\n",
    "- toxigen:  large-scale and machine-generated dataset of 274,186 toxic and benign statements about 13 minority groups.  https://paperswithcode.com/dataset/toxigen\n",
    "- unscramble: Unscramble is a small battery of 5 “character manipulation” tasks. Each task involves giving the model a word distorted by some combination of scrambling, addition, or deletion of characters, and asking it to recover the original word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ec2-user/SageMaker/gai-finetuning/lm-evaluation-harness'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_evaluation_git_dir = base_dir + \"/lm-evaluation-harness\"\n",
    "lm_evaluation_git_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_tuned_dir:/home/ec2-user/SageMaker/models/falcon-7b-tuned/\n"
     ]
    }
   ],
   "source": [
    "print(f\"model_tuned_dir:{model_tuned_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_file = \"eval_tuned.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select and launch the tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tasks = \"hendrycks_ethics\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m2023-07-24:04:08:41,819 INFO     [utils.py:148] Note: NumExpr detected 48 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "2023-07-24:04:08:41,819 INFO     [utils.py:160] NumExpr defaulting to 8 threads.\n",
      "2023-07-24:04:08:42,760 WARNING  [__init__.py:52] Failed to load config in\n",
      "                                 /home/ec2-user/SageMaker/gai-finetuning/lm-evaluation-harness/lm_eval/tasks/hendrycks_ethics/utilitarianism_original.yaml\n",
      "                                 Config will not be added to registry\n",
      "                                 Error: argument of type 'NoneType' is not iterable\n",
      "2023-07-24:04:08:42,975 INFO     [instantiator.py:21] Created a temporary directory at /tmp/tmphtatndxe\n",
      "2023-07-24:04:08:42,975 INFO     [instantiator.py:76] Writing /tmp/tmphtatndxe/_remote_module_non_scriptable.py\n",
      "2023-07-24:04:08:43,053 INFO     [main.py:135] Selected Tasks: ['hendrycks_ethics']\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:05<00:00,  2.97s/it]\n",
      "2023-07-24:04:08:55,405 WARNING  [task.py:568] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2023-07-24:04:08:55,405 WARNING  [task.py:580] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "Downloading builder script: 100%|██████████| 8.97k/8.97k [00:00<00:00, 16.2MB/s]\n",
      "Downloading data: 100%|████████████████████| 35.6M/35.6M [00:01<00:00, 29.6MB/s]\n",
      "100%|███████████████████████████████████████████| 2/2 [00:00<00:00, 1249.61it/s]\n",
      "2023-07-24:04:09:00,382 WARNING  [task.py:568] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2023-07-24:04:09:00,382 WARNING  [task.py:580] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "100%|███████████████████████████████████████████| 2/2 [00:00<00:00, 1447.56it/s]\n",
      "2023-07-24:04:09:02,105 WARNING  [task.py:568] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2023-07-24:04:09:02,105 WARNING  [task.py:580] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "100%|███████████████████████████████████████████| 2/2 [00:00<00:00, 1371.36it/s]\n",
      "2023-07-24:04:09:03,891 WARNING  [task.py:568] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2023-07-24:04:09:03,891 WARNING  [task.py:580] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "100%|███████████████████████████████████████████| 2/2 [00:00<00:00, 1183.66it/s]\n",
      "2023-07-24:04:09:05,596 WARNING  [task.py:568] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2023-07-24:04:09:05,596 WARNING  [task.py:580] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "100%|███████████████████████████████████████████| 2/2 [00:00<00:00, 1433.46it/s]\n",
      "2023-07-24:04:09:06,958 INFO     [task.py:357] Building contexts for task 'ethics_virtue' on rank 0...\n",
      "2023-07-24:04:09:09,383 INFO     [evaluator.py:210] Task: ethics_virtue; number of requests on this rank: 9950\n",
      "2023-07-24:04:09:09,383 INFO     [task.py:357] Building contexts for task 'ethics_justice' on rank 0...\n",
      "2023-07-24:04:09:10,451 INFO     [evaluator.py:210] Task: ethics_justice; number of requests on this rank: 5408\n",
      "2023-07-24:04:09:10,451 INFO     [task.py:357] Building contexts for task 'ethics_deontology' on rank 0...\n",
      "2023-07-24:04:09:12,719 INFO     [evaluator.py:210] Task: ethics_deontology; number of requests on this rank: 7192\n",
      "2023-07-24:04:09:12,720 INFO     [task.py:357] Building contexts for task 'ethics_cm' on rank 0...\n",
      "2023-07-24:04:09:14,206 INFO     [evaluator.py:210] Task: ethics_cm; number of requests on this rank: 7770\n",
      "2023-07-24:04:09:14,206 INFO     [task.py:357] Building contexts for task 'ethics_utilitarianism' on rank 0...\n",
      "2023-07-24:04:09:14,595 INFO     [evaluator.py:210] Task: ethics_utilitarianism; number of requests on this rank: 9616\n",
      "2023-07-24:04:09:14,596 INFO     [evaluator.py:246] Running loglikelihood requests\n",
      "100%|█████████████████████████████████████| 39936/39936 [16:10<00:00, 41.15it/s]\n"
     ]
    }
   ],
   "source": [
    "#to monitor GPU Memory usage on notebook instance, open a separate terminal and run: watch -n 0.4 nvidia-smi \n",
    "!cd evaluation_scripts && ./evaluation.sh hf $lm_evaluation_git_dir $model_tuned_dir $tasks > $output_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract json from output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_path = \"./evaluation_scripts/\" + output_file\n",
    "\n",
    "#quick and dirty way to extract json in the midst of the text file\n",
    "def extract_json(output_path):\n",
    "    try:\n",
    "        with open(output_path, 'r') as file:\n",
    "                data_str = file.read()\n",
    "                pos1 = int(data_str.find(\"{\"))\n",
    "                pos2 = int(data_str.rfind(\"}\"))\n",
    "                return json.loads(data_str[pos1:pos2+1])\n",
    "    except Exception as e:\n",
    "        print(f\"cannot extract json from lm evaluation output, error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "json_obj = extract_json(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ethics_virtue': {'acc,none': 0.43537688442211053,\n",
       "  'acc_stderr,none': 0.00703006143345469},\n",
       " 'ethics_justice': {'acc,none': 0.5088757396449705,\n",
       "  'acc_stderr,none': 0.009615647725764407},\n",
       " 'ethics_deontology': {'acc,none': 0.5150166852057843,\n",
       "  'acc_stderr,none': 0.008335364597109167},\n",
       " 'ethics_cm': {'acc,none': 0.5917631917631918,\n",
       "  'acc_stderr,none': 0.007886611421325077},\n",
       " 'ethics_utilitarianism': {'acc,none': 0.5214226289517471,\n",
       "  'acc_stderr,none': 0.007204999520618655}}"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_obj['results']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update model's metadata with results in model registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ethics_virtue_acc': '0.43537688442211053',\n",
       " 'ethics_virtue_acc_stderr': '0.00703006143345469',\n",
       " 'ethics_justice_acc': '0.5088757396449705',\n",
       " 'ethics_justice_acc_stderr': '0.009615647725764407',\n",
       " 'ethics_deontology_acc': '0.5150166852057843',\n",
       " 'ethics_deontology_acc_stderr': '0.008335364597109167',\n",
       " 'ethics_cm_acc': '0.5917631917631918',\n",
       " 'ethics_cm_acc_stderr': '0.007886611421325077',\n",
       " 'ethics_utilitarianism_acc': '0.5214226289517471',\n",
       " 'ethics_utilitarianism_acc_stderr': '0.007204999520618655'}"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#formatting the metrics as metadata to be stored in the model registry with the model\n",
    "acc_none = 'acc,none'\n",
    "acc_stderr = 'acc_stderr,none'\n",
    "\n",
    "metadataProperties = {}\n",
    "for key, value in json_obj['results'].items():\n",
    "    metadataProperties[key + \"_acc\"] = str(json_obj['results'][key][acc_none])\n",
    "    metadataProperties[key + \"_acc_stderr\"] = str(json_obj['results'][key][acc_stderr])\n",
    "\n",
    "metadataProperties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_package_update_response = sm_client.update_model_package(ModelPackageArn=model_package_arn, CustomerMetadataProperties=metadataProperties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarisation evaluation/metrics - ROUGE\n",
    "\n",
    "ROUGE, or Recall-Orientpip install rouge-scoreed Understudy for Gisting Evaluation, is a set of metrics and a software package used for evaluating automatic summarization and machine translation software in natural language processing. The metrics compare an automatically produced summary or translation against a reference or a set of references (human-produced) summary or translation.\n",
    "\n",
    "In the ROUGE paper, two flavors of ROUGE are described:\n",
    "\n",
    "\n",
    "    sentence-level: Compute longest common subsequence (LCS) between two pieces of text. Newlines are ignored. This is called rougeL in this package.\n",
    "    summary-level: Newlines in the text are interpreted as sentence boundaries, and the LCS is computed between each pair of reference and candidate sentences, and something called union-LCS is computed. This is called rougeLsum in this package. \n",
    "\n",
    "https://github.com/google-research/google-research/tree/master/rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install rouge-score --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "examples_text = \"\"\"Wine comedy up for six film gongs. Sideways, a wine-tasting comedy starring Paul Giamatti, is up for six Independent Spirit Awards, the art-house version of the Oscars.The awards are held on 26 February, the day before the Oscars. Spanish drama Maria Full of Grace, about a Colombian woman who becomes a drug courier, got five nominations. Controversial biopic Kinsey, starring Liam Neeson as sex researcher Alfred Kinsey, was one of four films to get four nominations. The awards, now in their 20th year, honour quirky low-budget films, all of which must have a degree of independent financing. Sideways is written and directed by Alexander Payne, who directed the 2002 hit About Schmidt, winning Jack Nicholson his 12th Academy Award nomination.\"These awards, for better or worse, mean everything,\" said Sideways producer Michael London, adding they were a \"huge first step\" toward getting recognition from other awards. Among the other films receiving four nominations apiece were Brother to Brother, a drama about a young gay black man forced to live on the streets, Robbing Peter and Primer. Primer, a $7,000 (£3,650) tale of discovery, won top prize at the Sundance film festival earlier this year. Walter Salles critically acclaimed The Motorcycle Diaries and the forthcoming thriller The Woodsman, starring Kevin Bacon, received three nominations each. Also in the running, with two nominations, are high school comedy Napoleon Dynamite, The Door in the Floor and Garden State - written, directed and starring Scrubs star Zach Braff alongside Natalie Portman. The awards were announced by actors Selma Blair and Dennis Quaid in Los Angeles on Tuesday.\"\"\"\n",
    "summary = \"\"\"Sideways, a wine-tasting comedy starring Paul Giamatti, is up for six Independent Spirit Awards, the art-house version of the Oscars.Sideways is written and directed by Alexander Payne, who directed the 2002 hit About Schmidt, winning Jack Nicholson his 12th Academy Award nomination.Controversial biopic Kinsey, starring Liam Neeson as sex researcher Alfred Kinsey, was one of four films to get four nominations.Among the other films receiving four nominations apiece were Brother to Brother, a drama about a young gay black man forced to live on the streets, Robbing Peter and Primer.Also in the running, with two nominations, are high school comedy Napoleon Dynamite, The Door in the Floor and Garden State - written, directed and starring Scrubs star Zach Braff alongside Natalie Portman.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Using default tokenizer.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rouge1': Score(precision=1.0, recall=0.47388059701492535, fmeasure=0.6430379746835443),\n",
       " 'rougeL': Score(precision=0.8503937007874016, recall=0.40298507462686567, fmeasure=0.5468354430379747),\n",
       " 'rougeLsum': Score(precision=0.8503937007874016, recall=0.40298507462686567, fmeasure=0.5468354430379747)}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL', 'rougeLsum'], use_stemmer=True)\n",
    "scores = scorer.score(examples_text, summary)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy an endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Next Steps \n",
    "\n",
    "You can deploy your fine-tuned model to a SageMaker endpoint and use it for inference. Check out the [Deploy Falcon 7B & 40B on Amazon SageMaker](https://www.philschmid.de/sagemaker-falcon-llm) and [Securely deploy LLMs inside VPCs with Hugging Face and Amazon SageMaker](https://www.philschmid.de/sagemaker-llm-vpc) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging cells  -  TO DELETE LATER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download from s3 to local\n",
    "s3_client.download_file(sagemaker_session_bucket, \n",
    "                        \"huggingface-qlora-2023-07-25-07-31-59-2023-07-25-07-32-09-436/output/model.tar.gz\", \n",
    "                       \"/home/ec2-user/SageMaker/models/falcon-7b-tuned/model.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch_model-00002-of-00002.bin\n",
      "generation_config.json\n",
      "pytorch_model-00001-of-00002.bin\n",
      "pytorch_model.bin.index.json\n",
      "config.json\n"
     ]
    }
   ],
   "source": [
    "!tar -xvf /home/ec2-user/SageMaker/models/falcon-7b-tuned/model.tar.gz --directory /home/ec2-user/SageMaker/models/falcon-7b-tuned/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#s3_client.upload_file(\"/home/ec2-user/SageMaker/models/test-new-py-file/model.tar.gz\", sagemaker_session_bucket, \"model-fine-tuning/models/test-new-py-file/model.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "import sagemaker\n",
    "import json\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "test_endpoint_name = \"test2\"\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "huggingface_model_test = HuggingFaceModel(\n",
    "   model_data=\"s3://sagemaker-us-east-1-327216439222/huggingface-qlora-2023-07-25-07-31-59-2023-07-25-07-32-09-436/output/model.tar.gz\",\n",
    "   role=role, \n",
    "   transformers_version=\"4.28.1\", \n",
    "   pytorch_version=\"2.0.0\", \n",
    "   py_version=\"py310\",\n",
    "   model_server_workers=1,\n",
    ")\n",
    "\n",
    "# deploy model to SageMaker Inference\n",
    "predictor_test = huggingface_model_test.deploy(\n",
    "    initial_instance_count=1,\n",
    "    #instance_type= \"local_gpu\",\n",
    "    instance_type= \"ml.g5.12xlarge\",\n",
    "    wait=False,\n",
    "    endpoint_name=test_endpoint_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictor_test.endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarize the following text:\n",
      "Ad sales boost Time Warner profit. Quarterly profits at US media giant TimeWarner jumped 76% to $1.13bn (£600m) for the three months to December, from $639m year-earlier.The firm, which is now one of the biggest investors in Google, benefited from sales of high-speed internet connections and higher advert sales. TimeWarner said fourth quarter sales rose 2% to $11.1bn from $10.9bn. Its profits were buoyed by one-off gains which offset a profit dip at Warner Bros, and less users for AOL.Time Warner said on Friday that it now owns 8% of search-engine Google. But its own internet business, AOL, had has mixed fortunes. It lost 464,000 subscribers in the fourth quarter profits were lower than in the preceding three quarters. However, the company said AOL's underlying profit before exceptional items rose 8% on the back of stronger internet advertising revenues. It hopes to increase subscribers by offering the online service free to TimeWarner internet customers and will try to sign up AOL's existing customers for high-speed broadband. TimeWarner also has to restate 2000 and 2003 results following a probe by the US Securities Exchange Commission (SEC), which is close to concluding.Time Warner's fourth quarter profits were slightly better than analysts' expectations. But its film division saw profits slump 27% to $284m, helped by box-office flops Alexander and Catwoman, a sharp contrast to year-earlier, when the third and final film in the Lord of the Rings trilogy boosted results. For the full-year, TimeWarner posted a profit of $3.36bn, up 27% from its 2003 performance, while revenues grew 6.4% to $42.09bn. \"Our financial performance was strong, meeting or exceeding all of our full-year objectives and greatly enhancing our flexibility,\" chairman and chief executive Richard Parsons said. For 2005, TimeWarner is projecting operating earnings growth of around 5%, and also expects higher revenue and wider profit margins. TimeWarner is to restate its accounts as part of efforts to resolve an inquiry into AOL by US market regulators. It has already offered to pay $300m to settle charges, in a deal that is under review by the SEC. The company said it was unable to estimate the amount it needed to set aside for legal reserves, which it previously set at $500m. It intends to adjust the way it accounts for a deal with German music publisher Bertelsmann's purchase of a stake in AOL Europe, which it had reported as advertising revenue. It will now book the sale of its stake in AOL Europe as a loss on the value of that stake.\n",
      "---\n",
      "Summary:\n",
      "\n"
     ]
    },
    {
     "ename": "ModelError",
     "evalue": "An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (400) from primary with message \"{\n  \"code\": 400,\n  \"type\": \"InternalServerException\",\n  \"message\": \"Loading /opt/ml/model requires you to execute the configuration file in that repo on your local machine. Make sure you have read the code there to avoid malicious use, then set the option `trust_remote_code\\u003dTrue` to remove this error.\"\n}\n\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/test2 in account 327216439222 for more information.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModelError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(prompt)\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mquery_llm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[22], line 14\u001b[0m, in \u001b[0;36mquery_llm\u001b[0;34m(payload, endpoint_name)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mquery_llm\u001b[39m(payload, endpoint_name):\n\u001b[0;32m---> 14\u001b[0m     query_response \u001b[38;5;241m=\u001b[39m \u001b[43mquery_endpoint_with_json_payload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendpoint_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parse_response_model(query_response)\n",
      "Cell \u001b[0;32mIn[22], line 3\u001b[0m, in \u001b[0;36mquery_endpoint_with_json_payload\u001b[0;34m(encoded_json, endpoint_name, content_type)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mquery_endpoint_with_json_payload\u001b[39m(encoded_json, endpoint_name, content_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapplication/json\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m      2\u001b[0m     client \u001b[38;5;241m=\u001b[39m boto3\u001b[38;5;241m.\u001b[39mclient(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mruntime.sagemaker\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke_endpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mEndpointName\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mContentType\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontent_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoded_json\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/botocore/client.py:530\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    526\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    527\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpy_operation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m() only accepts keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    528\u001b[0m     )\n\u001b[1;32m    529\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 530\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/botocore/client.py:964\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    962\u001b[0m     error_code \u001b[38;5;241m=\u001b[39m parsed_response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCode\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    963\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m--> 964\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m    965\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    966\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parsed_response\n",
      "\u001b[0;31mModelError\u001b[0m: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (400) from primary with message \"{\n  \"code\": 400,\n  \"type\": \"InternalServerException\",\n  \"message\": \"Loading /opt/ml/model requires you to execute the configuration file in that repo on your local machine. Make sure you have read the code there to avoid malicious use, then set the option `trust_remote_code\\u003dTrue` to remove this error.\"\n}\n\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/test2 in account 327216439222 for more information."
     ]
    }
   ],
   "source": [
    "print(prompt)\n",
    "print(query_llm(payload, \"test2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
